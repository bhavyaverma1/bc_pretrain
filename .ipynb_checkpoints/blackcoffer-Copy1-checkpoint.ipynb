{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4af39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6253f77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract list of urls from input file\n",
    "input_files_url='https://docs.google.com/spreadsheets/d/1fBx_dmkWVias5UVBIJw_zGdh46GcHcEB/edit#gid=959784854'\n",
    "input_files_url= input_files_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "df=pd.read_csv(input_files_url)\n",
    "url_list = df[df.columns[1]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a46b8d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Selenium Setup\n",
    "import sys\n",
    "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('chromedriver',options=chrome_options)\n",
    "\n",
    "# # Loop to be added for every url\n",
    "# driver.get(url_list[0])\n",
    "# article_title=driver.find_element(By.TAG_NAME,\"h1\").text\n",
    "# article_body=driver.find_element(By.CLASS_NAME, \"td-post-content\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e270ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Uncomment to create article files for further analysis if not exist inside an articles folder\n",
    "\n",
    "# for i in df.index:\n",
    "#     driver.get(df['URL'][i])\n",
    "#     article_title=driver.find_element(By.TAG_NAME,\"h1\").text\n",
    "#     article_body=driver.find_element(By.CLASS_NAME, \"td-post-content\").text\n",
    "#     filename='articles/'+str(df['URL_ID'][i])+'.txt'\n",
    "#     with open(filename,\"w+\") as text_file:\n",
    "#         print('Writing File '+str(df['URL_ID'][i]))\n",
    "#         text_file.write(article_title+'\\n'+article_body)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c30db",
   "metadata": {},
   "source": [
    "## Textual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ac2ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Creating list of StopWords and combining them all\n",
    "swlist=[]\n",
    "with open('StopWords/StopWords_Names.txt') as file: # Names\n",
    "    sw_names=word_tokenize(file.read())\n",
    "    for i in sw_names:\n",
    "        if i.isupper():\n",
    "            swlist.append(i)\n",
    "            \n",
    "with open('StopWords/StopWords_Geographic.txt') as file: # Geographic\n",
    "    sw_geo=word_tokenize(file.read())\n",
    "    for i in sw_geo:\n",
    "        if i.isupper():\n",
    "            swlist.append(i)\n",
    "            \n",
    "with open('StopWords/StopWords_GenericLong.txt') as file: # Generic Long\n",
    "    sw_genl=word_tokenize(file.read())\n",
    "    swlist.extend(sw_genl)\n",
    "\n",
    "with open('StopWords/StopWords_Generic.txt') as file: # Generic\n",
    "    sw_gen=word_tokenize(file.read())\n",
    "    swlist.extend(sw_gen)\n",
    "\n",
    "with open('StopWords/StopWords_DatesandNumbers.txt') as file: # Dates\n",
    "    sw_dn=word_tokenize(file.read())\n",
    "    for i in sw_dn:\n",
    "        if i.isupper():\n",
    "            swlist.append(i)\n",
    "\n",
    "# For currencies need to use iso encoding to not run into unicodedecode error\n",
    "with open('StopWords/StopWords_Currencies.txt',encoding='iso-8859-15') as file: # Currencies\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    sw_curr=tokenizer.tokenize(file.read())\n",
    "    for i in sw_curr:\n",
    "        if i.isalnum():\n",
    "            swlist.append(i)\n",
    "            \n",
    "with open('StopWords/StopWords_Auditor.txt') as file: # Auditor\n",
    "    sw_auditor=word_tokenize(file.read())\n",
    "    for i in sw_auditor:\n",
    "        if i.isupper():\n",
    "            swlist.append(i)\n",
    "            \n",
    "sw_lower=[x.lower() for x in swlist]\n",
    "# print((sw_lower[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3baa77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Fetching Master Dictionary\n",
    "pos_words=[]\n",
    "with open('MasterDictionary/positive-words.txt') as file: # positive Words\n",
    "    pos_words=word_tokenize(file.read())\n",
    "\n",
    "neg_words=[]\n",
    "with open('MasterDictionary/negative-words.txt',encoding='iso-8859-15') as file: # negative Words\n",
    "    neg_words=word_tokenize(file.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ad2cb",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e817be34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_article_from_id(url_id):    \n",
    "    with open('articles/'+str(url_id)+\".txt\") as file:\n",
    "        article_text=file.read()\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8256806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_tokenizer(article):\n",
    "    tk = WhitespaceTokenizer()\n",
    "    punc=\"!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \"\n",
    "    tokens=tk.tokenize(article)\n",
    "    std_tokens=[word.strip(punc) for word in tokens]\n",
    "    return std_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c9b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_tokenizer(article):\n",
    "    article_tokens=standard_tokenizer(article)\n",
    "    clean_text=[]\n",
    "    for i in article_tokens:\n",
    "        target = i.lower()\n",
    "        if target in sw_lower:\n",
    "            pass\n",
    "        else:\n",
    "            clean_text.append(i)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cccc0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text_tokenizer2(article):\n",
    "#     article_tokens=word_tokenize(article)\n",
    "#     clean_text=[]\n",
    "#     for i in article_tokens:\n",
    "#         target = i.lower()\n",
    "#         if target in sw_lower:\n",
    "#             pass\n",
    "#         else:\n",
    "#             if target.isalnum():\n",
    "#                 clean_text.append(i)\n",
    "#     return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f0df30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_calc(clean_text,pos_words,neg_words):\n",
    "    # Positive and Negative Scores\n",
    "    pos_score,neg_score=0,0\n",
    "    for i in clean_text:\n",
    "        if i.lower() in pos_words:\n",
    "            pos_score+=1\n",
    "        if i.lower() in neg_words:\n",
    "            neg_score+=1\n",
    "            \n",
    "    # Polarity     \n",
    "    polarity=(pos_score-neg_score)/(neg_score+pos_score+0.000001)\n",
    "    \n",
    "    # Subjectivity Score\n",
    "    num_words=len(clean_text)\n",
    "    subj_score=(pos_score+neg_score)/(num_words+0.000001)\n",
    "    \n",
    "    return pos_score, neg_score, polarity, subj_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc380f",
   "metadata": {},
   "source": [
    "### 2. Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa3901ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sen_len(article,std_text):\n",
    "    num_sent=len(sent_tokenize(article))\n",
    "    num_words=len(std_text)\n",
    "    asl=num_words/num_sent\n",
    "    return asl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d15cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_comp_words(std_text):\n",
    "    vowels = \"AaEeIiOoUu\"\n",
    "    complex_count=0\n",
    "    for word in std_text:\n",
    "        target=word\n",
    "        vcount=0\n",
    "        if word.endswith('es') or word.endswith('ed'):\n",
    "            target=word[:-2]\n",
    "        for alphabet in target:\n",
    "            # If alphabet is a vowel\n",
    "            if alphabet in vowels:\n",
    "                vcount += 1\n",
    "        if vcount>2:\n",
    "            complex_count+=1\n",
    "    #         print(target)\n",
    "    num_words=len(std_text)\n",
    "    perc=complex_count/num_words*100\n",
    "    return perc,complex_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7db076c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fog_index(article,std_text):\n",
    "    asl=avg_sen_len(article,std_text)\n",
    "    perc,complex_count=per_comp_words(std_text)\n",
    "    fogi=0.4*(asl+perc)\n",
    "    return fogi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bc56964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_num_words(article,std_text):\n",
    "    return avg_sen_len(article,std_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6282386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word_count(clean_text):\n",
    "    return len(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62e03970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_syl_count(std_text):\n",
    "    vowels = \"AaEeIiOoUu\"\n",
    "    total_vcount=0\n",
    "    for word in std_text:\n",
    "        target=word\n",
    "        vcount=0\n",
    "        if word.endswith('es') or word.endswith('ed'):\n",
    "            target=word[:-2]\n",
    "        for alphabet in target:\n",
    "            # If alphabet is a vowel\n",
    "            if alphabet in vowels:\n",
    "                vcount += 1\n",
    "        total_vcount+=vcount\n",
    "    #         print(target)\n",
    "    num_words=len(std_text)\n",
    "    asc=total_vcount/num_words\n",
    "    return asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8708a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pers_pro_count(std_text):\n",
    "    p_count=0\n",
    "    pronouns=['i','we','my','ours','us']\n",
    "    for i in std_text:\n",
    "        if i=='US':\n",
    "            continue\n",
    "        target=i.lower()\n",
    "        if target in pronouns:\n",
    "            p_count+=1\n",
    "    return p_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6faebdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_len(std_text):\n",
    "    num_char=0\n",
    "    for i in std_text:\n",
    "        num_char+=len(i)\n",
    "    num_words=len(std_text)\n",
    "    awl=num_char/num_words\n",
    "    return awl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051c251",
   "metadata": {},
   "source": [
    "## Analysis of all the 150 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "890c25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files_url='https://docs.google.com/spreadsheets/d/1GQ_akhFuLyDYob1y7m_wAEmR9-hSr89q/edit#gid=2010199760'\n",
    "output_files_url= output_files_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "out_df=pd.read_csv(output_files_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4cf5359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: 1\n",
      "Processing article: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21694/2388514123.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[2]][i], out_df[out_df.columns[3]][i], out_df[out_df.columns[4]][i], out_df[out_df.columns[5]][i]=scores_calc(clean_text,pos_words,neg_words)\n",
      "/tmp/ipykernel_21694/2388514123.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[6]][i] = avg_sen_len(article,std_text)\n",
      "/tmp/ipykernel_21694/2388514123.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[7]][i], out_df[out_df.columns[10]][i] = per_comp_words(std_text)\n",
      "/tmp/ipykernel_21694/2388514123.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[8]][i] = fog_index(article,std_text)\n",
      "/tmp/ipykernel_21694/2388514123.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[9]][i] = avg_num_words(article,std_text)\n",
      "/tmp/ipykernel_21694/2388514123.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[11]][i] = clean_word_count(clean_text)\n",
      "/tmp/ipykernel_21694/2388514123.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[12]][i] = avg_syl_count(std_text)\n",
      "/tmp/ipykernel_21694/2388514123.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[13]][i] = pers_pro_count(std_text)\n",
      "/tmp/ipykernel_21694/2388514123.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  out_df[out_df.columns[14]][i] = avg_word_len(std_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article: 3\n",
      "Processing article: 4\n",
      "Processing article: 5\n",
      "Processing article: 6\n",
      "Processing article: 7\n",
      "Processing article: 8\n",
      "Processing article: 9\n",
      "Processing article: 10\n",
      "Processing article: 11\n",
      "Processing article: 12\n",
      "Processing article: 13\n",
      "Processing article: 14\n",
      "Processing article: 15\n",
      "Processing article: 16\n",
      "Processing article: 17\n",
      "Processing article: 18\n",
      "Processing article: 19\n",
      "Processing article: 20\n",
      "Processing article: 21\n",
      "Processing article: 22\n",
      "Processing article: 23\n",
      "Processing article: 24\n",
      "Processing article: 25\n",
      "Processing article: 26\n",
      "Processing article: 27\n",
      "Processing article: 28\n",
      "Processing article: 29\n",
      "Processing article: 30\n",
      "Processing article: 31\n",
      "Processing article: 32\n",
      "Processing article: 33\n",
      "Processing article: 34\n",
      "Processing article: 35\n",
      "Processing article: 36\n",
      "Processing article: 37\n",
      "Processing article: 38\n",
      "Processing article: 39\n",
      "Processing article: 40\n",
      "Processing article: 41\n",
      "Processing article: 42\n",
      "Processing article: 43\n",
      "Processing article: 44\n",
      "Processing article: 45\n",
      "Processing article: 46\n",
      "Processing article: 47\n",
      "Processing article: 48\n",
      "Processing article: 49\n",
      "Processing article: 50\n",
      "Processing article: 51\n",
      "Processing article: 52\n",
      "Processing article: 53\n",
      "Processing article: 54\n",
      "Processing article: 55\n",
      "Processing article: 56\n",
      "Processing article: 57\n",
      "Processing article: 58\n",
      "Processing article: 59\n",
      "Processing article: 60\n",
      "Processing article: 61\n",
      "Processing article: 62\n",
      "Processing article: 63\n",
      "Processing article: 64\n",
      "Processing article: 65\n",
      "Processing article: 66\n",
      "Processing article: 67\n",
      "Processing article: 68\n",
      "Processing article: 69\n",
      "Processing article: 70\n",
      "Processing article: 71\n",
      "Processing article: 72\n",
      "Processing article: 73\n",
      "Processing article: 74\n",
      "Processing article: 75\n",
      "Processing article: 76\n",
      "Processing article: 77\n",
      "Processing article: 78\n",
      "Processing article: 79\n",
      "Processing article: 80\n",
      "Processing article: 81\n",
      "Processing article: 82\n",
      "Processing article: 83\n",
      "Processing article: 84\n",
      "Processing article: 85\n",
      "Processing article: 86\n",
      "Processing article: 87\n",
      "Processing article: 88\n",
      "Processing article: 89\n",
      "Processing article: 90\n",
      "Processing article: 91\n",
      "Processing article: 92\n",
      "Processing article: 93\n",
      "Processing article: 94\n",
      "Processing article: 95\n",
      "Processing article: 96\n",
      "Processing article: 97\n",
      "Processing article: 98\n",
      "Processing article: 99\n",
      "Processing article: 100\n",
      "Processing article: 101\n",
      "Processing article: 102\n",
      "Processing article: 103\n",
      "Processing article: 104\n",
      "Processing article: 105\n",
      "Processing article: 106\n",
      "Processing article: 107\n",
      "Processing article: 108\n",
      "Processing article: 109\n",
      "Processing article: 110\n",
      "Processing article: 111\n",
      "Processing article: 112\n",
      "Processing article: 113\n",
      "Processing article: 114\n",
      "Processing article: 115\n",
      "Processing article: 116\n",
      "Processing article: 117\n",
      "Processing article: 118\n",
      "Processing article: 119\n",
      "Processing article: 120\n",
      "Processing article: 121\n",
      "Processing article: 122\n",
      "Processing article: 123\n",
      "Processing article: 124\n",
      "Processing article: 125\n",
      "Processing article: 126\n",
      "Processing article: 127\n",
      "Processing article: 128\n",
      "Processing article: 129\n",
      "Processing article: 130\n",
      "Processing article: 131\n",
      "Processing article: 132\n",
      "Processing article: 133\n",
      "Processing article: 134\n",
      "Processing article: 135\n",
      "Processing article: 136\n",
      "Processing article: 137\n",
      "Processing article: 138\n",
      "Processing article: 139\n",
      "Processing article: 140\n",
      "Processing article: 141\n",
      "Processing article: 142\n",
      "Processing article: 143\n",
      "Processing article: 144\n",
      "Processing article: 145\n",
      "Processing article: 146\n",
      "Processing article: 147\n",
      "Processing article: 148\n",
      "Processing article: 149\n",
      "Processing article: 150\n"
     ]
    }
   ],
   "source": [
    "for i in out_df.index:\n",
    "    \n",
    "    url_id=out_df['URL_ID'][i]\n",
    "    print('Processing article: '+str(url_id))\n",
    "    article=get_article_from_id(url_id)\n",
    "    std_text=standard_tokenizer(article)\n",
    "    clean_text=clean_text_tokenizer(article)\n",
    "    \n",
    "    out_df[out_df.columns[2]][i], out_df[out_df.columns[3]][i], out_df[out_df.columns[4]][i], out_df[out_df.columns[5]][i]=scores_calc(clean_text,pos_words,neg_words)\n",
    "    out_df[out_df.columns[6]][i] = avg_sen_len(article,std_text)\n",
    "    out_df[out_df.columns[7]][i], out_df[out_df.columns[10]][i] = per_comp_words(std_text)\n",
    "    out_df[out_df.columns[8]][i] = fog_index(article,std_text)\n",
    "    out_df[out_df.columns[9]][i] = avg_num_words(article,std_text)\n",
    "    out_df[out_df.columns[11]][i] = clean_word_count(clean_text)\n",
    "    out_df[out_df.columns[12]][i] = avg_syl_count(std_text)\n",
    "    out_df[out_df.columns[13]][i] = pers_pro_count(std_text)\n",
    "    out_df[out_df.columns[14]][i] = avg_word_len(std_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "77ded5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.109510</td>\n",
       "      <td>26.692308</td>\n",
       "      <td>33.141210</td>\n",
       "      <td>23.933407</td>\n",
       "      <td>26.692308</td>\n",
       "      <td>230.0</td>\n",
       "      <td>347.0</td>\n",
       "      <td>2.047550</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-telehealt...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.124365</td>\n",
       "      <td>17.944444</td>\n",
       "      <td>26.212590</td>\n",
       "      <td>17.662814</td>\n",
       "      <td>17.944444</td>\n",
       "      <td>254.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>1.868937</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.024768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telemedici...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.490909</td>\n",
       "      <td>0.122631</td>\n",
       "      <td>18.755319</td>\n",
       "      <td>33.635848</td>\n",
       "      <td>20.956467</td>\n",
       "      <td>18.755319</td>\n",
       "      <td>593.0</td>\n",
       "      <td>897.0</td>\n",
       "      <td>2.167896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.667045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/is-telehealth...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.079581</td>\n",
       "      <td>20.482759</td>\n",
       "      <td>34.792368</td>\n",
       "      <td>22.110051</td>\n",
       "      <td>20.482759</td>\n",
       "      <td>620.0</td>\n",
       "      <td>955.0</td>\n",
       "      <td>2.159933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.748036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-people-di...</td>\n",
       "      <td>76.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.394495</td>\n",
       "      <td>0.120843</td>\n",
       "      <td>22.654321</td>\n",
       "      <td>31.062670</td>\n",
       "      <td>21.486797</td>\n",
       "      <td>22.654321</td>\n",
       "      <td>570.0</td>\n",
       "      <td>902.0</td>\n",
       "      <td>2.088283</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.580926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0       1  https://insights.blackcoffer.com/is-telehealth...            30.0   \n",
       "1       2  https://insights.blackcoffer.com/how-telehealt...            33.0   \n",
       "2       3  https://insights.blackcoffer.com/is-telemedici...            82.0   \n",
       "3       4  https://insights.blackcoffer.com/is-telehealth...            54.0   \n",
       "4       5  https://insights.blackcoffer.com/how-people-di...            76.0   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0             8.0        0.578947            0.109510            26.692308   \n",
       "1            16.0        0.346939            0.124365            17.944444   \n",
       "2            28.0        0.490909            0.122631            18.755319   \n",
       "3            22.0        0.421053            0.079581            20.482759   \n",
       "4            33.0        0.394495            0.120843            22.654321   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                    33.141210  23.933407                         26.692308   \n",
       "1                    26.212590  17.662814                         17.944444   \n",
       "2                    33.635848  20.956467                         18.755319   \n",
       "3                    34.792368  22.110051                         20.482759   \n",
       "4                    31.062670  21.486797                         22.654321   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0               230.0       347.0           2.047550                0.0   \n",
       "1               254.0       394.0           1.868937                0.0   \n",
       "2               593.0       897.0           2.167896                1.0   \n",
       "3               620.0       955.0           2.159933                0.0   \n",
       "4               570.0       902.0           2.088283                1.0   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0         5.531700  \n",
       "1         5.024768  \n",
       "2         5.667045  \n",
       "3         5.748036  \n",
       "4         5.580926  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36c9aabd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>18.693878</td>\n",
       "      <td>26.310044</td>\n",
       "      <td>18.001568</td>\n",
       "      <td>18.693878</td>\n",
       "      <td>241.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>1.909389</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.398472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.065596</td>\n",
       "      <td>22.652174</td>\n",
       "      <td>26.359565</td>\n",
       "      <td>19.604696</td>\n",
       "      <td>22.652174</td>\n",
       "      <td>412.0</td>\n",
       "      <td>747.0</td>\n",
       "      <td>1.887396</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.150992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-0.232877</td>\n",
       "      <td>0.122896</td>\n",
       "      <td>17.590909</td>\n",
       "      <td>29.371232</td>\n",
       "      <td>18.784856</td>\n",
       "      <td>17.590909</td>\n",
       "      <td>341.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>1.932817</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.118863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.094737</td>\n",
       "      <td>24.166667</td>\n",
       "      <td>33.103448</td>\n",
       "      <td>22.908046</td>\n",
       "      <td>24.166667</td>\n",
       "      <td>240.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>2.088276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.702069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>-0.101449</td>\n",
       "      <td>0.138833</td>\n",
       "      <td>15.969697</td>\n",
       "      <td>23.529412</td>\n",
       "      <td>15.799643</td>\n",
       "      <td>15.969697</td>\n",
       "      <td>248.0</td>\n",
       "      <td>497.0</td>\n",
       "      <td>1.889943</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.044592</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "145     146  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "146     147  https://insights.blackcoffer.com/the-future-of...   \n",
       "147     148  https://insights.blackcoffer.com/big-data-anal...   \n",
       "148     149  https://insights.blackcoffer.com/business-anal...   \n",
       "149     150  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "     POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "145            22.0            26.0       -0.083333            0.111111   \n",
       "146            37.0            12.0        0.510204            0.065596   \n",
       "147            28.0            45.0       -0.232877            0.122896   \n",
       "148            32.0             4.0        0.777778            0.094737   \n",
       "149            31.0            38.0       -0.101449            0.138833   \n",
       "\n",
       "     AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "145            18.693878                    26.310044  18.001568   \n",
       "146            22.652174                    26.359565  19.604696   \n",
       "147            17.590909                    29.371232  18.784856   \n",
       "148            24.166667                    33.103448  22.908046   \n",
       "149            15.969697                    23.529412  15.799643   \n",
       "\n",
       "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "145                         18.693878               241.0       432.0   \n",
       "146                         22.652174               412.0       747.0   \n",
       "147                         17.590909               341.0       594.0   \n",
       "148                         24.166667               240.0       380.0   \n",
       "149                         15.969697               248.0       497.0   \n",
       "\n",
       "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
       "145           1.909389                9.0         5.398472  \n",
       "146           1.887396                2.0         5.150992  \n",
       "147           1.932817                2.0         5.118863  \n",
       "148           2.088276                0.0         5.702069  \n",
       "149           1.889943                8.0         5.044592  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "721931e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('output.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9860ad78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_object = pd.ExcelWriter('output_excel.xlsx') # requires openpyxl\n",
    "out_df.to_excel(pd_object, index=False)\n",
    " \n",
    "pd_object.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
